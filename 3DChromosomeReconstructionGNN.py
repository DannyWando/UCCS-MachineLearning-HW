# -*- coding: utf-8 -*-
"""GNNHi-C.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L6qXFIUDuuMUNY4p-ZSMLV91QZAfTWHv
"""

# Commented out IPython magic to ensure Python compatibility.
# %env LD_LIBRARY_PATH = /usr/local/cuda-10.1

# Commented out IPython magic to ensure Python compatibility.
!git clone -l -s https://github.com/shenweichen/GraphEmbedding.git LINE
# %cd LINE
!ls

!python setup.py install

# Run this one now 
!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html
!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html
!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html
!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html
!pip install torch-geometric

import torch_geometric

!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html
!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html
!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html
!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html
!pip install torch-geometric

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow as tf
from ge import LINE
import torch
import torch_geometric
import pdb
import numpy as np
import scipy as sp
from scipy.special import erf
from scipy.stats import spearmanr
import sklearn
import numpy
from sklearn.metrics.pairwise import euclidean_distances
from numpy import triu
import networkx as nx
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pylab as plt
from torch_geometric.data import Data
from torch_geometric.nn import MessagePassing, Sequential
import torch_geometric.transforms as T
from typing import Union, Tuple
from torch_geometric.utils import remove_self_loops
from torch_geometric.typing import OptPairTensor, Adj, Size, OptTensor
from torch import Tensor
import torch.nn as nn
from torch.nn import Linear, ReLU, MSELoss, PairwiseDistance, Conv1d
from torch.optim import Adam, SGD
import torch.nn.functional as F
from torch_sparse import SparseTensor, matmul
from math import pi, exp, sqrt
from mpl_toolkits import mplot3d
import matplotlib.pyplot as plt
import pickle
import IPython

from google.colab import drive
drive.mount('/content/drive')

def knightRuizAlg(A, tol=1e-6, f1 = False):
    n = A.shape[0]
    e = np.ones((n,1), dtype = np.float64)
    res = []


    Delta = 3
    delta = 0.1
    x0 = np.copy(e)
    g = 0.9

    etamax = eta = 0.1
    stop_tol = tol*0.5
    x = np.copy(x0)

    rt = tol**2.0
    v = x * (A.dot(x))
    rk = 1.0 - v
    rho_km1 = ((rk.transpose()).dot(rk))[0,0]
    rho_km2 = rho_km1
    rout = rold = rho_km1
    
    MVP = 0 #we'll count matrix vector products
    i = 0 #outer iteration count

    if f1:
        print ("it        in. it      res\n"),

    while rout > rt: #outer iteration
        i += 1

        if i > 30:
            break


        k = 0
        y = np.copy(e)
        innertol = max(eta ** 2.0 * rout, rt)
        
        while rho_km1 > innertol: #inner iteration by CG
            k += 1
            if k == 1:
                Z = rk / v
                p = np.copy(Z)
                #rho_km1 = np.dot(rk.T, Z)
                rho_km1 = (rk.transpose()).dot(Z)
            else:
                beta = rho_km1 / rho_km2
                p = Z + beta * p

            if k > 10:
                break





            #update search direction efficiently
            w = x * A.dot(x * p) + v * p
           # alpha = rho_km1 / np.dot(p.T, w)[0,0]
            alpha = rho_km1 / (((p.transpose()).dot(w))[0,0])
            ap = alpha * p
            #test distance to boundary of cone
            ynew = y + ap
            
            if np.amin(ynew) <= delta:
                
                if delta == 0:
                    break

                ind = np.where(ap < 0.0)[0]
                gamma = np.amin((delta - y[ind]) / ap[ind])
                y += gamma * ap
                break

            if np.amax(ynew) >= Delta:
                ind = np.where(ynew > Delta)[0]
                gamma = np.amin((Delta - y[ind]) / ap[ind])
                y += gamma * ap
                break

            y = np.copy(ynew)
            rk -= alpha * w
            rho_km2 = rho_km1
            Z = rk / v
            #rho_km1 = np.dot(rk.T, Z)[0,0]
            rho_km1 = ((rk.transpose()).dot(Z))[0,0]
        x *= y
        v = x * (A.dot(x))
        rk = 1.0 - v
        #rho_km1 = np.dot(rk.T, rk)[0,0]
        rho_km1 = ((rk.transpose()).dot(rk))[0,0]
        rout = rho_km1
        MVP += k + 1
        
        #update inner iteration stopping criterion
        rat = rout/rold
        rold = rout
        res_norm = rout ** 0.5
        eta_o = eta
        eta = g * rat
        if g * eta_o ** 2.0 > 0.1:
            eta = max(eta, g * eta_o ** 2.0)
        eta = max(min(eta, etamax), stop_tol / res_norm)
        if f1:
            print ("%03i %06i %03.3f %e %e \n") % \
                (i, k, res_norm, rt, rout), 
            res.append(res_norm)
    if f1:
        print ("Matrix - vector products = %06i\n") % \
            (MVP),
    
    X = np.diag(x[:,0])   
    x = X.dot(A.dot(X))
    return x

def load_input(input, features,normalize):
  adj_mat = np.loadtxt(input)
  np.fill_diagonal(adj_mat,0)
  if normalize == True:
    adj_mat = knightRuizAlg(adj_mat)
  #mean  = np.mean(adj_mat)
  #std_dev = np.std(adj_mat)
  #truth = (adj_mat-mean)/std_dev
  #mx = np.max(adj_mat)
  #mn= np.min(adj_mat)
  truth = adj_mat
  '''truth = (adj_mat - mn)/(mx-mn)
  truth = torch.tensor(truth,dtype=torch.float32)'''
  truth = torch.tensor(truth,dtype=torch.double)
  graph = nx.from_numpy_matrix(adj_mat).to_undirected()
  num_nodes = adj_mat.shape[0]
  edges = list(graph.edges(data=True))
  #node_features = np.random.rand(num_nodes,features)

  edge_index = np.empty((len(edges),2))
  edge_weights = np.empty((len(edges)))
  nodes = np.empty(num_nodes)
  #node_attr = np.empty((num_nodes,features))

  for i in range(len(edges)):
    edge_index[i] = np.asarray(edges[i][0:2])
    edge_weights[i] = np.asarray(edges[i][2]["weight"])

  for i in range(num_nodes):
    nodes[i] = np.asarray(i)
    '''node_attr[i] = node_features[i]'''

  edge_index = torch.tensor(edge_index, dtype=torch.long)
  edge_weights = torch.tensor(edge_weights, dtype=torch.float)
  nodes = torch.tensor(nodes, dtype=torch.long)
  node_attr = torch.tensor(features)

  edge_index=edge_index.t().contiguous()

  
  mask = edge_index[0] != edge_index[1]
  inv_mask = ~mask

  edge_index = edge_index[:, mask]
  edge_attr = edge_weights[mask]

  #labels = torch.tensor(np.random.randint(0,high = 10, size=(num_nodes,1))).long()

  data = Data(x=node_attr,edge_index = edge_index, edge_attr = edge_attr, y = truth)

  device = torch.device('cuda:0')
  data = data.to(device)
 
  adj = SparseTensor(row=data.edge_index[0], col=data.edge_index[1] , value= data.edge_attr, sparse_sizes=(num_nodes, num_nodes))
  data.edge_index = adj.to_symmetric()
  data.edge_attr = None
  return data

class SAGEConv(MessagePassing):
    """The GraphSAGE operator from the `"Inductive Representation Learning on
    Large Graphs" <https://arxiv.org/abs/1706.02216>`_ paper

    .. math::
        \mathbf{x}^{\prime}_i = \mathbf{W}_1 \mathbf{x}_i + \mathbf{W}_2 \cdot
        \mathrm{mean}_{j \in \mathcal{N(i)}} \mathbf{x}_j

    Args:
        in_channels (int or tuple): Size of each input sample. A tuple
            corresponds to the sizes of source and target dimensionalities.
        out_channels (int): Size of each output sample.
        normalize (bool, optional): If set to :obj:`True`, output features
            will be :math:`\ell_2`-normalized, *i.e.*,
            :math:`\frac{\mathbf{x}^{\prime}_i}
            {\| \mathbf{x}^{\prime}_i \|_2}`.
            (default: :obj:`False`)
        root_weight (bool, optional): If set to :obj:`False`, the layer will
            not add transformed root node features to the output.
            (default: :obj:`True`)
        bias (bool, optional): If set to :obj:`False`, the layer will not learn
            an additive bias. (default: :obj:`True`)
        **kwargs (optional): Additional arguments of
            :class:`torch_geometric.nn.conv.MessagePassing`.
    """
    def __init__(self, in_channels: Union[int, Tuple[int, int]],
                 out_channels: int, normalize: bool = False,
                 root_weight: bool = True,
                 bias: bool = True, **kwargs):  # yapf: disable
        kwargs.setdefault('aggr', 'add')
        super(SAGEConv, self).__init__(**kwargs)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.normalize = normalize
        self.root_weight = root_weight
        
        if isinstance(in_channels, int):
            in_channels = (in_channels, in_channels)

        self.lin_l = Linear(in_channels[0], out_channels, bias=bias)
        if self.root_weight:
            self.lin_r = Linear(in_channels[1], out_channels, bias=False)

        self.reset_parameters
        
    def reset_parameters(self):
        self.lin_l.reset_parameters()
        if self.root_weight:
          self.lin_r.reset_parameters()

    def adjust_weights(self, adj_t):
      device = torch.device('cuda')
      sum_vec = adj_t.sum(dim=0)
      numerator = torch.ones(len(sum_vec), device=device)
      inverse = torch.divide(numerator, sum_vec)
      size = len(sum_vec)

      row = torch.arange(size, dtype=torch.long, device=device)
      index = torch.stack([row, row], dim=0)

      value = torch.tensor(inverse, dtype=torch.float32, device=device)
      normalized = SparseTensor(row=index[0], col=index[1], value=value)
      #pdb.set_trace()
      norm_mat = matmul(normalized, adj_t)
      return norm_mat


    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj, edge_attr: OptTensor = None,
                size: Size = None) -> Tensor:
        if isinstance(x, Tensor):
          x: OptPairTensor = (x, x)
        
        # propagate_type: (x: OptPairTensor)
        #pdb.set_trace()
        out = self.propagate(edge_index, x=x, size=size)
        #pdb.set_trace()
        out = self.lin_l(out.float())
        x_r = x[1].long()
      
        if self.root_weight and x_r is not None:
            out += self.lin_r(x_r.float())

        if self.normalize:
            out = F.normalize(out, p=2., dim=-1)
        

        return out

    def message_and_aggregate(self, adj_t: SparseTensor,
                              x: OptPairTensor) -> Tensor:

        norm_mat = self.adjust_weights(adj_t)
        #pdb.set_trace()
        return  matmul(norm_mat, x[0], reduce=self.aggr)

    def __repr__(self):
        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,
                                   self.out_channels)

class ToDistance(torch.nn.Module):
    def forward(self, x):
      x = x.cpu()
      x = x.detach().numpy()
      x = sp.sparse.coo_matrix(x)
      return  torch.tensor(euclidean_distances(x), device=device, requires_grad=True)

class Scale(torch.nn.Module):
  def __init__(self):
    super(Scale, self).__init__()

    self.scale_parameter = nn.Parameter(torch.rand(1,requires_grad=True,dtype = float))

  def forward(self,x):
    x = x*self.scale_parameter
    return x

class FillDiag(torch.nn.Module):
  def forward(self,x):
    cont = x 
    cont.fill_diagonal(0)
    return cont

class ToContact(torch.nn.Module):
  def forward(self, x): 
    cont = (1/(x+1e-5))**2
    cont.fill_diagonal_(0)
    return cont

def cont2dist(adj,factor):
  dist = (1/adj)**factor
  dist.fill_diagonal_(0)
  max = torch.max(torch.nan_to_num(dist,posinf=0))
  dist = torch.nan_to_num(dist,posinf=max)
  return dist/max

adj = np.loadtxt('/content/drive/MyDrive/HiC/Synthetic/chainDres5_Matrix_noise000.txt')
np.fill_diagonal(adj,0)
G = nx.from_numpy_matrix(adj)

embed = LINE(G,embedding_size=128,order='second')
embed.train(batch_size=1064,epochs=10,verbose=1)
embeddings = embed.get_embeddings()
embeddings = list(embeddings.values())
embeddings = np.asarray(embeddings)

data = load_input('/content/drive/MyDrive/HiC/Synthetic/chainDres5_Matrix_noise000.txt',embeddings,True)
data.x.shape

class Net(nn.Module):
    def __init__(self):
      super(Net, self).__init__()
      self.conv = SAGEConv(128, 128)
      self.densea = Linear(128,64)
      self.dense1 = Linear(64,32)
      self.dense2 = Linear(32,10)
      self.dense3 = Linear(10,3)
    
    def forward(self, x, edge_index):

      #pdb.set_trace()
      x = self.conv(x, edge_index)
      x = x.relu()
      x = self.densea(x)
      x = x.relu()
      x = self.dense1(x)
      x = x.relu()
      x = self.dense2(x)
      x = x.relu()
      x = self.dense3(x)
      x = torch.cdist(x, x, p=2)

      return x

    def get_model(self, x, edge_index):
      x = self.conv(x, edge_index)
      x = x.relu()
      x = self.dense1(x)
      x = x.relu()
      x = self.dense2(x)
      x = x.relu()
      x = self.dense3(x)

      return x

torch.autograd.set_detect_anomaly(True)
model = Net()
criterion = MSELoss()
optimizer = Adam(
    model.parameters(), lr=.001)

device = torch.device('cuda')
model.to(device)

truth = cont2dist(data.y,.5).float()
oldloss = 1 
lossdiff = 1
while lossdiff > 1e-7:
  model.train()
  optimizer.zero_grad()  # Clear gradients.
  out = model(data.x, data.edge_index)  # Perform a single forward pass.
  #pdb.set_trace()
  loss = criterion(out, truth)
  # ===================backward====================
  #pdb.set_trace()
  lossdiff = abs(oldloss - loss)
  loss.backward()
  optimizer.step()
  print(lossdiff)
  oldloss = loss
  #print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')

test = model.get_model(data.x,data.edge_index)
test = test.cpu().detach().numpy()
fig = plt.figure()
ax = plt.axes(projection='3d')
#ax.scatter3D(test[:,0], test[:,1], test[:,2]);
ax.plot(test[:,0], test[:,1], test[:,2], color='r')

idx = torch.triu_indices(data.y.shape[0],data.y.shape[1],offset=1)
dist_truth = truth[idx[0,:],idx[1,:]].cpu().detach().numpy()
test = model.get_model(data.x,data.edge_index)
out = torch.cdist(test,test)
dist_out = out[idx[0,:],idx[1,:]].cpu().detach().numpy()

hi = spearmanr(dist_truth,dist_out)
hi[0]

idx1 = torch.nonzero(torch.triu(truth))
dist_truth = truth[idx1[:,0],idx1[:,1]]

test = model.get_model(data.x,data.edge_index)

out = torch.cdist(test,test)
idx2 = torch.nonzero(torch.triu(out))
dist_out = out[idx2[:,0],idx2[:,1]]

results = []
conversions = np.arange(start=.1, stop=2, step=.1)

for i in range(23):
  chromosome = i+1 
  print(chromosome)
  filename = '/content/drive/MyDrive/HiC/250kb/'+'chr'+str(chromosome)+'_matrix.txt'

  adj = np.loadtxt(filename)
  np.fill_diagonal(adj,0)
  G = nx.from_numpy_matrix(adj)

  embed = LINE(G,embedding_size=512,order='second')
  embed.train(batch_size=10024,epochs=10,verbose=1)
  embeddings = embed.get_embeddings()
  embeddings = list(embeddings.values())
  embeddings = np.asarray(embeddings)

  data = load_input(filename,embeddings,True)

  tempmodels = []
  tempspear = []
  print('embeddings generated')
  class Net(nn.Module):
    def __init__(self):
      super(Net, self).__init__()
      self.conv = SAGEConv(512, 512)
      self.densea = Linear(512,256)
      self.dense1 = Linear(256,128)
      self.dense2 = Linear(128,64)
      self.dense3 = Linear(64,3)
    
    def forward(self, x, edge_index):

      #pdb.set_trace()
      x = self.conv(x, edge_index)
      x = x.relu()
      x = self.densea(x)
      x = x.relu()
      x = self.dense1(x)
      x = x.relu()
      x = self.dense2(x)
      x = x.relu()
      x = self.dense3(x)
      x = torch.cdist(x, x, p=2)

      return x

    def get_model(self, x, edge_index):
      x = self.conv(x, edge_index)
      x = x.relu()
      x = self.densea(x)
      x = x.relu()
      x = self.dense1(x)
      x = x.relu()
      x = self.dense2(x)
      x = x.relu()
      x = self.dense3(x)

      return x 

  for conversion in conversions:
    print(conversion)
    truth = cont2dist(data.y,conversion).float()

    torch.autograd.set_detect_anomaly(True)
    model = Net()
    criterion = MSELoss()
    optimizer = Adam(
    model.parameters(), lr=.001)

    device = torch.device('cuda')
    model.to(device)
    oldloss = 1 
    lossdiff = 1

    while lossdiff > 1e-8:
      model.train()
      optimizer.zero_grad() 
      out = model(data.x, data.edge_index)
      #pdb.set_trace()
      loss = criterion(out, truth)
      # ===================backward====================
      #pdb.set_trace()
      lossdiff = abs(oldloss - loss)
      loss.backward()
      optimizer.step()
      oldloss = loss

    idx = torch.triu_indices(data.y.shape[0],data.y.shape[1],offset=1)
    dist_truth = truth[idx[0,:],idx[1,:]].cpu().detach().numpy()
    coords = model.get_model(data.x,data.edge_index)
    out = torch.cdist(coords,coords)
    dist_out = out[idx[0,:],idx[1,:]].cpu().detach().numpy()
    SpRho = spearmanr(dist_truth,dist_out)[0]

    tempspear.append(SpRho)
    tempmodels.append(coords)

  idx = tempspear.index(max(tempspear))
  repmod = tempmodels[idx]
  repspear = tempspear[idx]
  repconv = conversions[idx]
  print(repspear)
  print(repconv)
  results.append((repmod,repspear,repconv))

with open('/content/drive/MyDrive/HiC/250kb/Results.pkl', 'wb') as f:
    pickle.dump(results, f)

with open('/content/drive/MyDrive/HiC/250kb/Results.pkl', "rb") as input_file:
    test = pickle.load(input_file)
test.shape

hi = [test[i][1] for i in range(23)]
hi.index(min([test[i][1] for i in range(23)]))
test[8][1]

results = []
conversions = [.2,.3,.4]

filename = '/content/drive/MyDrive/HiC/250kb/'+'chr'+str(9)+'_matrix.txt'

adj = np.loadtxt(filename)
np.fill_diagonal(adj,0)
G = nx.from_numpy_matrix(adj)

for k in range(10):
  embed = LINE(G,embedding_size=512,order='second')
  embed.train(batch_size=10024,epochs=10,verbose=1)
  embeddings = embed.get_embeddings()
  embeddings = list(embeddings.values())
  embeddings = np.asarray(embeddings)

  data = load_input(filename,embeddings,True)

  tempmodels = []
  tempspear = []
  print('embeddings generated')
  class Net(nn.Module):
    def __init__(self):
      super(Net, self).__init__()
      self.conv = SAGEConv(512, 512)
      self.densea = Linear(512,256)
      self.dense1 = Linear(256,128)
      self.dense2 = Linear(128,64)
      self.dense3 = Linear(64,3)
    
    def forward(self, x, edge_index):

      #pdb.set_trace()
      x = self.conv(x, edge_index)
      x = x.relu()
      x = self.densea(x)
      x = x.relu()
      x = self.dense1(x)
      x = x.relu()
      x = self.dense2(x)
      x = x.relu()
      x = self.dense3(x)
      x = torch.cdist(x, x, p=2)

      return x

    def get_model(self, x, edge_index):
      x = self.conv(x, edge_index)
      x = x.relu()
      x = self.densea(x)
      x = x.relu()
      x = self.dense1(x)
      x = x.relu()
      x = self.dense2(x)
      x = x.relu()
      x = self.dense3(x)

      return x 

  
  for conversion in conversions:
    print(conversion)
    truth = cont2dist(data.y,conversion).float()

    torch.autograd.set_detect_anomaly(True)
    model = Net()
    criterion = MSELoss()
    optimizer = Adam(
    model.parameters(), lr=.001)

    device = torch.device('cuda')
    model.to(device)
    oldloss = 1 
    lossdiff = 1

    while lossdiff > 1e-8:
      model.train()
      optimizer.zero_grad() 
      out = model(data.x, data.edge_index)
      #pdb.set_trace()
      loss = criterion(out, truth)
      # ===================backward====================
      #pdb.set_trace()
      lossdiff = abs(oldloss - loss)
      loss.backward()
      optimizer.step()
      oldloss = loss

    idx = torch.triu_indices(data.y.shape[0],data.y.shape[1],offset=1)
    dist_truth = truth[idx[0,:],idx[1,:]].cpu().detach().numpy()
    coords = model.get_model(data.x,data.edge_index)
    out = torch.cdist(coords,coords)
    dist_out = out[idx[0,:],idx[1,:]].cpu().detach().numpy()
    SpRho = spearmanr(dist_truth,dist_out)[0]

    tempspear.append(SpRho)
    tempmodels.append(coords)

  idx = tempspear.index(max(tempspear))
  repmod = tempmodels[idx]
  repspear = tempspear[idx]
  repconv = conversions[idx]
  print(repspear)
  print(repconv)
  results.append((repmod,repspear,repconv))
  print(repspear)

display(IPython.display.Audio(url="https://www.soundjay.com/misc/sounds/wind-chime-1.mp3", autoplay=True))

tempspear

# Synthetic test
embedding_sched = {}
embedding_sched[64]= [64,32,16,8]
embedding_sched[128]= [128,64,32,16]
embedding_sched[256] = [256,128,64,32]
embedding_sched[512] = [512,256,128,64]
filename = '/content/drive/MyDrive/HiC/Synthetic/chainDres5_Matrix_noise000.txt'
conversions = np.arange(start=.1, stop=2, step=.1)
truth = np.loadtxt('/content/drive/MyDrive/HiC/Synthetic/chainDres5_Matrix_noise000.dist')

embedding_results = {}

for i in [64,128,256,512]:
  print(i)
  tempspear = []
  adj = np.loadtxt(filename)
  np.fill_diagonal(adj,0)
  G = nx.from_numpy_matrix(adj)

  embed = LINE(G,embedding_size= i,order='second')
  embed.train(batch_size=1024,epochs=10,verbose=0)
  embeddings = embed.get_embeddings()
  embeddings = list(embeddings.values())
  embeddings = np.asarray(embeddings)

  data = load_input(filename,embeddings,True)

  print('Embeddings Generated')
  sched = embedding_sched[i]
  class Net(nn.Module):
    def __init__(self):
      super(Net, self).__init__()
      self.conv = SAGEConv(sched[0], sched[0])
      self.densea = Linear(sched[0],sched[1])
      self.dense1 = Linear(sched[1],sched[2])
      self.dense2 = Linear(sched[2],sched[3])
      self.dense3 = Linear(sched[3],3)
    
    def forward(self, x, edge_index):

      #pdb.set_trace()
      x = self.conv(x, edge_index)
      x = x.relu()
      x = self.densea(x)
      x = x.relu()
      x = self.dense1(x)
      x = x.relu()
      x = self.dense2(x)
      x = x.relu()
      x = self.dense3(x)
      x = torch.cdist(x, x, p=2)

      return x

    def get_model(self, x, edge_index):
      x = self.conv(x, edge_index)
      x = x.relu()
      x = self.densea(x)
      x = x.relu()
      x = self.dense1(x)
      x = x.relu()
      x = self.dense2(x)
      x = x.relu()
      x = self.dense3(x)

      return x 

  #for conversion in conversions:
    #print(conversion)
  truth = cont2dist(data.y,1).float()
  torch.autograd.set_detect_anomaly(True)
  model = Net()
  criterion = MSELoss()
  optimizer = Adam(
  model.parameters(), lr=.001)

  device = torch.device('cuda')
  model.to(device)
  oldloss = 1 
  lossdiff = 1
  while lossdiff > 1e-8:
    model.train()
    optimizer.zero_grad()  # Clear gradients.
    out = model(data.x, data.edge_index)  # Perform a single forward pass.
    #pdb.set_trace()
    loss = criterion(out, truth)
    # ===================backward====================
    #pdb.set_trace()
    lossdiff = abs(oldloss - loss)
    loss.backward()
    optimizer.step()
    oldloss = loss
    #print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')

  idx = torch.triu_indices(data.y.shape[0],data.y.shape[1],offset=1)
  dist_truth = truth[idx[0,:],idx[1,:]].cpu().detach().numpy()
  coords = model.get_model(data.x,data.edge_index)
  out = torch.cdist(coords,coords)
  dist_out = out[idx[0,:],idx[1,:]].cpu().detach().numpy()
  #results.append(coords) #delete later 
  SpRho = spearmanr(dist_truth,dist_out)[0]

  tempspear.append(SpRho)

  idx = tempspear.index(max(tempspear))
  repspear = tempspear[idx]
  repconv = conversions[idx]
  print(repspear) 
  embedding_results[i] = (repconv, repspear)

with open('/content/drive/MyDrive/HiC/Synthetic/Results.pkl', 'wb') as f:
    pickle.dump(embedding_results, f)

